{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/acpopescu/bitsandbytes/releases/download/v0.37.2-win.1/bitsandbytes-0.37.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "# pip install https://github.com/acpopescu/bitsandbytes/releases/download/v0.37.2-win.1/bitsandbytes-0.37.2-py3-none-any.whl\n",
    "# pip install torch transformers accelerate\n",
    "# pip install bitsandbytes-windows\n",
    "# pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cemticsai\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "# torch.cuda.is_available()\n",
    "# torch.cuda.device_count()\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.get_device_name(0)\n",
    "# torch.cuda.get_device_name(1)\n",
    "\n",
    "# use an A100 on Colab Pro (or any system with >30GB VRAM on your own machine) to load this in bf16\n",
    "# if not available, use a GPU with minimum 20GB VRAM to load this in 8bit, or with minimum 12GB of VRAM to load in 4bit\n",
    "# on Colab, works with a V100 but crashes on a T4\n",
    "\n",
    "# downloading the model and then loading it to memory step takes around 10 minutes the first time. Be patient :)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def clear_ram_cache():\n",
    "    psutil.virtual_memory().available  # This can help free up some cached memory\n",
    "\n",
    "# Call the function to attempt to clear RAM cache\n",
    "clear_ram_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data and load it to Database\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\cemticsai\\Documents\\Projects\\text2sql\\SQL code\\SQL data\\Cell_level_daily.csv\")\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect('cell_wise.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Create the table\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS cell_wise_daily (\n",
    "  date DATE,\n",
    "  circle VARCHAR(50) PRIMARY KEY, -- Unique ID for each circle\n",
    "  airtel_circle_id VARCHAR(50),\n",
    "  airtel_site_id VARCHAR(50),\n",
    "  airtel_cell_id VARCHAR(50), \n",
    "  freq_kind VARCHAR(50),\n",
    "  bandwidth DECIMAL(10, 2),\n",
    "  cell_technology VARCHAR(50),\n",
    "  vendor VARCHAR(50),\n",
    "  district VARCHAR(50),\n",
    "  Data_Volume_DL_24_Hr DECIMAL(10, 2),\n",
    "  Data_Volume_UL_24_Hr DECIMAL(10, 2),\n",
    "  Data_Drop_Call_Rate_24_Hr DECIMAL(10, 2),\n",
    "  Cell_Outage_24_hr DECIMAL(10, 2))''')\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "\n",
    "df.to_sql('cell_wise_daily', conn, if_exists='replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['11-12-2020', '12-12-2020', '13-12-2020'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary c:\\Users\\cemticsai\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cemticsai\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: C:\\Users\\cemticsai\\anaconda3\\envs\\torchgpu did not contain cudart64_110.dll as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\Users\\cemticsai\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Users/cemticsai/anaconda3/envs/torchgpu/Library/usr/bin'), WindowsPath('C:/Users/cemticsai/anaconda3/envs/torchgpu/Library/mingw-w64/bin'), WindowsPath('C:/Users/cemticsai/anaconda3/envs/torchgpu/bin')}\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Fetching model\n",
    "\n",
    "model_name = r\"C:\\Users\\cemticsai\\Documents\\Projects\\text2sql\\SQL code\\SQL project\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True,\n",
    "    #load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True,\n",
    "    offload_folder=\"offload\", torch_dtype=torch.float16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Define a regular expression pattern to match yyyy-mm-dd dates\n",
    "pattern = r'(\\d{4})-(\\d{2})-(\\d{2})'\n",
    "\n",
    "# Function to replace matched dates with dd-mm-yyyy format\n",
    "def replace_date(match):\n",
    "    year, month, day = match.groups()\n",
    "    return f'{day}-{month}-{year}'\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the mapping dictionary from the JSON file\n",
    "with open(\"mapping_dict.json\", \"r\") as json_file:\n",
    "    mapping_dict = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def text2sql(question):\n",
    "  # Sample input string\n",
    "  input_string = question\n",
    "\n",
    "  # Replace words according to the mapping dictionary\n",
    "  output_string = input_string\n",
    "  for key, replacement in mapping_dict.items():\n",
    "      output_string = output_string.replace(key, replacement)\n",
    "  question = output_string\n",
    "\n",
    "  prompt = \"\"\"### Instructions:\n",
    "  Your task is convert a question into a SQL query, given a Postgres database schema.\n",
    "  Adhere to these rules:\n",
    "  - Deliberately go through the question and database schema word by word to appropriately answer the question\n",
    "  - When creating a ratio, always cast the numerator as float\n",
    "  - when dl data volumn, or data volumn  replace it with Data_Volume_DL_24_Hr\n",
    "  - when ul data volumn replace it with Data_Volume_UL_24_Hr\n",
    "  - when drop rate or drop replace it Data_Drop_Call_Rate_24_Hr\n",
    "  - when district_level_daily table\n",
    "\n",
    "  ### Input:\n",
    "  Generate a SQL query that answers the question `{question}`.\n",
    "  This query will run on a database whose schema is represented in this string:\n",
    "\n",
    " CREATE TABLE cell_wise_daily (\n",
    "    date DATE,\n",
    "    district VARCHAR(50) PRIMARY KEY, -- Unique ID for each district\n",
    "    circle VARCHAR(50) CHECK (circle IN ('Delhi','Kolkata')),\n",
    "    airtel_circle_id VARCHAR(50),\n",
    "    airtel_site_id VARCHAR(50),\n",
    "    airtel_cell_id VARCHAR(50),\n",
    "    freq_kind VARCHAR(50),\n",
    "    bandwidth VARCHAR(50),\n",
    "    cell_technology VARCHAR(50),\n",
    "    vendor VARCHAR(50),\n",
    "    Data_Volume_DL_24_Hr DECIMAL(10, 2),\n",
    "    Data_Volume_UL_24_Hr DECIMAL(10, 2),\n",
    "    Data_Drop_Call_Rate_24_Hr DECIMAL(10, 2),\n",
    "    Cell_Outage_24_hr DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "  ### Response:\n",
    "  Based on your instructions, here is the SQL query I have generated to answer the question `{question}`:\n",
    "  ```sql\n",
    "  \"\"\".format(question=question)\n",
    "\n",
    "\n",
    "      ##################################\n",
    "\n",
    "  eos_token_id = tokenizer.convert_tokens_to_ids([\"```\"])[0]\n",
    "      # excruciatingly slow on an V100 with 4bit quantization\n",
    "      # takes around 1-2 minutes per query\n",
    "      # on a single A100 40GB, takes ~10-20 seconds\n",
    "  #torch.cuda.memory_summary(device=None, abbreviated=True)\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "  generated_ids = model.generate(\n",
    "          **inputs,\n",
    "          num_return_sequences=1,\n",
    "          eos_token_id=eos_token_id,\n",
    "          pad_token_id=eos_token_id,\n",
    "          max_new_tokens=300,\n",
    "          do_sample=False,\n",
    "          num_beams=5\n",
    "      )\n",
    "\n",
    "  outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "  t= outputs[0].split(\"```sql\")[-1].split(\"```\")[0].split(\";\")[0].strip() + \";\"\n",
    "  t1=t.replace(\"district_level_daily\",\"District_level_daily\").replace(\"        \", \"\\n\").replace(\"data_drop_call_rate_24_hr\",\"Data_Drop_Call_Rate_24_Hr\").\\\n",
    "    replace(\"\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\").\\\n",
    "      replace(\"\\n\\n\",\"\\n\").replace(\"airtel_airtel_cell_id\",\"airtel_cell_id\").\\\n",
    "      replace(\"cell_outage_24_hr\",\"Cell_Outage_24_hr \").replace(\"data_volume_dl_24_hr\",\"Data_Volume_DL_24_Hr \")\n",
    "  # t1 = re.sub(pattern, replace_date, t1)\n",
    "  print(\"\\n\",question)\n",
    "  return t1,outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " show me the count airtel_cell_ids by date\n",
      "SELECT cell_wise_daily.date,\n",
      "  count(cell_wise_daily.airtel_cell_id) AS cell_count\n",
      "   FROM   cell_wise_daily\n",
      "   WHERE  cell_wise_daily.airtel_cell_id IS NOT NULL\n",
      "   GROUP BY cell_wise_daily.date\n",
      "   ORDER BY cell_wise_daily.date;\n"
     ]
    }
   ],
   "source": [
    "# Run Query here\n",
    "\n",
    "text,outputs=text2sql(\"show me the count cells by date\")\n",
    "pattern = r'\\b(\\d{4})-(\\d{2})-(\\d{2})\\b'\n",
    "new_text = re.sub(pattern, r'\\3-\\2-\\1', text)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (17680126.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "\n",
    "c.execute('''SELECT cell_wise_daily.airtel_cell_id,\n",
    "  cell_wise_daily.Data_Drop_Call_Rate_24_Hr\n",
    "   FROM   cell_wise_daily\n",
    "   WHERE  cell_wise_daily.date >= '11-12-2020'\n",
    "      AND cell_wise_daily.date <= '13-12-2020'\n",
    "      AND cell_wise_daily.district = 'Delhi'\n",
    "   GROUP BY cell_wise_daily.airtel_cell_id, cell_wise_daily.date\n",
    "   ORDER BY cell_wise_daily.airtel_cell_id, cell_wise_daily.date;\n",
    "          \n",
    "           ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=t.replace(\"district_level_daily\",\"District_level_daily\").replace(\"        \", \"\\n\").replace(\"data_drop_call_rate_24_hr\",\"Data_Drop_Call_Rate_24_Hr\").\\\n",
    "    replace(\"cell_id\",\"airtel_cell_id\").replace(\"cell_wise_daily.district = 'Delhi'\",\"cell_wise_daily.circle = 'Delhi'\").\\\n",
    "    replace(\"cell_wise_daily.district = 'Kolkata'\",\"cell_wise_daily.circle = 'kolkata'\").replace(\"\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\").\\\n",
    "      replace(\"\\n\\n\",\"\\n\").replace(\"airtel_airtel_cell_id\",\"airtel_cell_id\").\\\n",
    "      replace(\"cell_outage_24_hr\",\"Cell_Outage_24_hr \").replace(\"data_volume_dl_24_hr\",\"Data_Volume_DL_24_Hr \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1127831922.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    SELECT cell_wise_daily.cell_name,\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "question = \"Show me the list for the cell names and their corresponding data Drop Call Rates for a given date \"\n",
    "\n",
    "SELECT cell_wise_daily.cell_name,\n",
    "       District_level_daily.Data_Drop_Call_Rate_24_Hr\n",
    "FROM   cell_wise_daily join District_level_daily on cell_wise_daily.district = District_level_daily.district\n",
    "ORDER BY cell_wise_daily.cell_name\n",
    "\n",
    "for row in c.fetchall():\n",
    "     print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Show me the list for the 4g district names and their corresponding data Drop Call Rates for a given date \"\n",
    "SELECT dl.district,\n",
    "       dl.Data_Drop_Call_Rate_24_Hr\n",
    "FROM   cell_wise_daily ccd join District_level_daily dl on ccd.district = dl.district\n",
    "WHERE  ccd.cell_technology = '4G'\n",
    "   and ccd.date = '2019-01-01'\n",
    "ORDER BY dl.Data_Drop_Call_Rate_24_Hr DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"show me the distinct circle outage\"\n",
    "\n",
    "c.execute('''\n",
    "SELECT distinct cell_wise_daily.circle,\n",
    "       cell_wise_daily.cell_outage_24_hr\n",
    "FROM   cell_wise_daily\n",
    "ORDER BY cell_wise_daily.cell_outage_24_hr DESC;\n",
    "           ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "     print (row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"show me total Data Volume by date\"\n",
    "\n",
    "c.execute('''\n",
    "SELECT cell_wise_daily.date,\n",
    "       sum(cell_wise_daily.data_volume_dl_24_hr) as total_data_volume\n",
    "FROM   cell_wise_daily\n",
    "GROUP BY cell_wise_daily.date\n",
    "ORDER BY cell_wise_daily.date;\n",
    "           ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "     print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"show me maximum bandwidth by vendor\"\n",
    "\n",
    "\n",
    "c.execute('''\n",
    "SELECT cell_wise_daily.vendor,\n",
    "       max(cell_wise_daily.bandwidth) as max_bandwidth\n",
    "FROM   cell_wise_daily\n",
    "GROUP BY cell_wise_daily.vendor\n",
    "ORDER BY max_bandwidth desc;\n",
    "           ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "     print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Find circles with a average dl volumn  greater than 20\"\n",
    "\n",
    "c.execute('''\n",
    "SELECT cell_wise_daily.circle\n",
    "FROM   cell_wise_daily\n",
    "GROUP BY cell_wise_daily.circle\n",
    "HAVING avg(cell_wise_daily.data_volume_dl_24_hr) > 20;\n",
    "           ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "     print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Find circles with more than 10 records and an average dl data volumn greater than 50mb\"\n",
    "\n",
    "\n",
    "c.execute('''\n",
    "SELECT cell_wise_daily.circle,\n",
    "       avg(cell_wise_daily.Data_Volume_DL_24_Hr) as avg_dl_data_volumn\n",
    "FROM   cell_wise_daily\n",
    "GROUP BY cell_wise_daily.circle\n",
    "HAVING count(*) > 10 and avg(cell_wise_daily.Data_Volume_DL_24_Hr) > 10\n",
    "ORDER BY avg_dl_data_volumn desc;\n",
    "           ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "     print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = r\"C:\\Users\\cemticsai\\Documents\\Projects\\text2sql\\SQL code\\SQL project\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True,\n",
    "    #load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True,\n",
    "    offload_folder=\"offload\", torch_dtype=torch.float16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"### Instructions:\n",
    "Your task is convert a question into a SQL query, given a Postgres database schema.\n",
    "Adhere to these rules:\n",
    "- *Deliberately go through the question and database schema word by word* to appropriately answer the question\n",
    "- When creating a ratio, always cast the numerator as float\n",
    "-when dl data volumn or data volumn  replace it with Data_Volume_DL_24_Hr\n",
    "-when ul data volumn replace it with Data_Volume_UL_24_Hr\n",
    "-when drop rate or drop replace it Data_Drop_Call_Rate_24_Hr\n",
    "-when district_level_daily table \n",
    "\n",
    "### Input:\n",
    "Generate a SQL query that answers the question `{question}`.\n",
    "This query will run on a database whose schema is represented in this string:\n",
    "\n",
    "CREATE TABLE cell_wise_daily (\n",
    "  date DATE,\n",
    "  district VARCHAR(50)  PRIMARY KEY, -- Unique ID for each district\n",
    "  circle VARCHAR(50) PRIMARY KEY, -- Unique ID for each circle\n",
    "  airtel_circle_id VARCHAR(50) \n",
    "  airtel_site_id VARCHAR(50) \n",
    "  airtel_cell_id VARCHAR(50)\n",
    "  freq_kind VARCHAR(50),\n",
    "  bandwidth DECIMAL(10, 2),\n",
    "  cell_technology VARCHAR(50),\n",
    "  vendor VARCHAR(50));\n",
    "\n",
    "\n",
    "CREATE TABLE District_level_daily (\n",
    "  date DATE,\n",
    "  district VARCHAR(50) PRIMARY KEY, -- Unique ID for each district\n",
    "  Data_Volume_DL_24_Hr DECIMAL(10, 2),\n",
    "  Data_Volume_UL_24_Hr DECIMAL(10, 2),\n",
    "  Data_Drop_Call_Rate_24_Hr DECIMAL(10, 2),\n",
    "  Cell_Outage_24_hr DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "CREATE TABLE Circle_level_daily (\n",
    "  date DATE,\n",
    "  circle VARCHAR(50) PRIMARY KEY, -- Unique ID for each circle\n",
    "  Data_Volume_DL_24_Hr DECIMAL(10, 2),\n",
    "  Data_Volume_UL_24_Hr DECIMAL(10, 2),\n",
    "  Data_Drop_Call_Rate_24_Hr DECIMAL(10, 2),\n",
    "  Cell_Outage_24_hr DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "\n",
    "Hint:\n",
    "You can use the JOIN clause to combine data from the two tables based on the district column.\n",
    "You can also use the WHERE clause to filter the results based on the freq_kind column. \n",
    "You can use the JOIN clause to combine data from the two tables based on the circle column.\n",
    "You can also use the WHERE clause to filter the results based on the technology column from cell_wise_daily. \n",
    "You can use the JOIN clause to combine data from the two tables based on the cell column.\n",
    "You can also use the WHERE clause to filter the results based on the freq_kind column from District_level_daily.\n",
    "\n",
    "### Response:\n",
    "Based on your instructions, here is the SQL query I have generated to answer the question `{question}`:\n",
    "```sql\n",
    "\"\"\".format(question=question)\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids([\"```\"])[0]\n",
    "# excruciatingly slow on an V100 with 4bit quantization\n",
    "# takes around 1-2 minutes per query\n",
    "# on a single A100 40GB, takes ~10-20 seconds\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=eos_token_id,\n",
    "    pad_token_id=eos_token_id,\n",
    "    max_new_tokens=150,\n",
    "    do_sample=False,\n",
    "    num_beams=1\n",
    ")\n",
    "\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "t= outputs[0].split(\"```sql\")[-1].split(\"```\")[0].split(\";\")[0].strip() + \";\"\n",
    "t1=t.replace(\"district_level_daily\",\"District_level_daily\")\n",
    "print(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
